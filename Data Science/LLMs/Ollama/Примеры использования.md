## ollama

```ts
import { Ollama } from 'ollama'

const ollama = new Ollama({ host: 'http://127.0.0.1:11434' })

const response = await ollama.generate({
  model: 'deepseek-v2',
  options: {
    seed: undefined, // Не используем фиксированное значение для не воспроизводимости
    num_keep: 50, // Сохраняем первые 50 токенов для лучшего контекста
    num_predict: 100, // Генерируем до 100 токенов для ответа
    top_k: 50, // Ограничиваем выборку до 50 наиболее вероятных токенов
    top_p: 0.95, // Используем nucleus sampling с вероятностью 0.95
    tfs_z: 1.0, // По умолчанию, можно настроить для специфических нужд
    typical_p: 1.0, // По умолчанию, можно настроить для специфических нужд
    repeat_last_n: 64, // Ограничиваем повторение последних 64 токенов
    temperature: 0.5, // Уменьшаем температуру для уменьшения вариативности
    repeat_penalty: 1.1, // Штрафуем за повторение токенов
    presence_penalty: 0.1, // Штрафуем за присутствие токенов
    frequency_penalty: 0.1, // Штрафуем за частоту токенов
    mirostat: 0, // Отключаем Mirostat, если не требуется
    mirostat_tau: 5.0, // Параметр регулировки Mirostat
    mirostat_eta: 0.1, // Параметр скорости обучения Mirostat
    penalize_newline: false, // Не штрафуем за перенос строки
    stop: [], // Не используем стоп-символы
    numa: false, // Отключаем NUMA, если не требуется
    num_ctx: 4096, // Увеличиваем контекст для лучшего понимания
    num_batch: 8, // Увеличиваем размер пакета для производительности
    num_gpu: 1, // Используем 1 GPU
    main_gpu: 1, // Основной GPU
    low_vram: false, // Не используем режим низкого VRAM
    f16_kv: true, // Используем 16-битные ключи и значения
    vocab_only: false, // Не ограничиваемся только словарем
    use_mmap: true, // Используем mmap для загрузки модели
    use_mlock: false, // Не используем mlock
    num_thread: 16, // Используем 16 потока для параллельной обработки
  },
  prompt: 'Когда родился Пушкин? Только число и дата',
})

console.log(response.response)
```

## langchain

```ts
import { Ollama } from '@langchain/community/llms/ollama'

const ollama = new Ollama({
  baseUrl: 'http://127.0.0.1:11434',
  model: 'llama3',
  temperature: 0.1,
})

const stream = await ollama.stream(
  context,
)

const chunks = []
for await (const chunk of stream) {
  chunks.push(chunk)

  console.log(chunks.join(''))
}

```

## ollama-node

```ts
import process from 'node:process'
import { Ollama } from 'ollama-node'

const ollama = new Ollama()
await ollama.setModel('llama3')

function print(word: string) {
  process.stdout.write(word)
}
await ollama.streamingGenerate(context, print)
```